Section 1
q2
Travis has a nice way of displaying when certain test cases for each method fail which makes it easier debug. The compiler can do only so much when it comes to identifying errors. I don't usually read much of the content that Travis gives back to me, although if I am really stuck, I will scroll all the way down to see which specific tests did not pass. I read through the Travis feedback and locate where things were going wrong. It is relatively easy to find the keywords associated with your modules and from there find the error. I check where the test fails and correct any problems. I would run through it by hand to see where in the method it went wrong. In the projects we would check what we failed and go to those lines of codes and test those methods to see what the issue was. Overall Travis helped solve quite a few of our smaller issues and was quite beneficial to us. Whenever my tests failed, I would look at what specific part of the test failed and then adjust that part in my code. Travis helped me find the parts of my code that did not line up with the expectation and go and create better more functional code I would go back to the program and try to find what could be the issue. I look at the methods travis tested to see where I failed and address debugging in those areas. I usually never had trouble with travis other than a few glitches here and there because if it compiles and runs correctly travis usually passes. I scroll through the Travis terminal, find where the test case failed, check the immediate feedback there, think about what it means about my code, then fix the problems. I would check to see which travis test I fail then go to the portion of code that test had to deal with and look over my code. Then I would test my code and run it until I though it was okay, then push it and check and see if it passed travis. If it didn't pass, I would repeat until it did. I would read the feedback and trace my code and test more before sending another test to travis. I look at what on the code has been labeled as an error, and then either look at previous labs and notes or go online to find what the problem may be and how to fix it. Travis told me what method was failing and what it returned, so then I knew what code to fix, and then fixed it. I would scroll down and try to find the error in my code. If I can't fix my mistake(s) by looking at the feedback, I would debug the code. And then run it again. It was unclear what was going wrong most of the time and so It was difficult to understand what was going wrong with my work 1. Learn the location of the error. 2. Locate the error in the code. 3. Read the problem/error description. 4. Critically think about why the problem is happening. 5. Attempt a fix. 6. If fix doesn't work, find the problem on stack overflow and read how other programmers fixed their issue. I wasn't sure how to read what Travis returned to me after it finished evaluating a submitted program. All I understood was if it failed or succeeded. I would notice the ones I did pass and compare it to the ones that failed for a specific method. Through this comparison, I would synthesize what aspects of my code was correct and what were the tweaks that needed to be made. Additionally, I would compare the expected result with what my program was output and start the debugging process at this point. Through looping back through, I would figure out why my program gave the wrong output and make adjusts to correct it. 
-----------------------------------------------------------
q3
I like how travis shows what method is causing the testcase failure and it shows what specific input caused it. This makes it easy to identify the problem and how to solve it. I like that after I finish a lab/project, I can use it as a second-check so I feel more secure that my code runs correctly. I personally just like having a system in place to make sure at the very least my code is working the way it needs to. I like that Travis can test different cases and give us feedback almost instantly. The feedback while not incredibly in depth showed us where our problems were so that we could figure out how to solve them ourselves. It worked fairly fast and I was able to get feedback on my code in a short amount of time. The automated feedback was very fast and it helped me feel confident when turning in a lab I like that you're able to know before submitting if your program is running correctly. It is great to get immediate feeback and a useful tool to double-check to see if my code is comprehensive and correct. It is also great to actually see what it tested for what methods. It automatically runs the test cases whenever the project is pushed onto github, and I don't have to go out of my way to test my program. It saved me a lot of time finding errors in that I normally wouldn't catch. (ex. on the last guess of the 20th game of Hangman) I really liked that it  tells you the cases it passed and what could be done better. It allows me to know for sure whether my code will work. It tells me what part of my code is broken I like that it's usually pretty quick and that it goes in detail about the error. It is easy and fast to use, which makes it time efficient It helps a ton, the test cases are the most helpful feedback possible, in my opinion(at least in a automated way). I like that it is there as an option and that it is a way to get quick feedback without having to go to an instructor for a test case you can't figure out. I liked that there were generated test cases readily available. It gave a base to where the code should be, which allowed me to structure my program in this direction. Without it, I could generate my own test cases, but I would take much longer in isolating specific places where the program could break. Moreover, this enabled me to focus afterward on specific areas that could break which greatly reduced the time spent debugging. In general, it also gave a sense of direction for what was wrong with my code, and what it should be outputting. 
-----------------------------------------------------------
q4
Sometimes travis can only catch general errors in the code. For example travis was reading that there were no test case errors for the program but when the program ran there were still errors in the output. Also sometimes travis shows errors in code when there actually were not errors but that is probably not the fault of the travis program. I don't like that it is so hard to interpret the feedback Travis gives. It would be helpful to have a sort of template that shows us what the results from Travis should look like/ what each result means specifically. I dislike the lack of specificity to the errors it gives. What I don't like about Travis is that it can only test cases given already in the test file and not test inputs that we can put in ourselves. It would be nice if there were a few notes on why some things failed. Like common mistakes with the method codes. There were times when it took a while for my code to be tested. The feedback could sometimes be a little confusing or fusteating if I didn’t understand the problem, it could use more clear feedback I had some issues with my program running and coming up with the correct output, but Travis kept telling me that my program failed. Firstly, at times it seems that travis doesn't truly test my whole program for bugs and overall functionality. I wish that it tested more cases and was more clear in areas I need to work on. In addition, as mentioned in class, it would help if travis were more effective in handling volume when it comes to checking submissions. There is a lot of unnecessary information within the travis terminal when a program is undergoing tests. Removing some of the initialization output would help increase readability. It was sometimes difficult to find which tests I failed, there's a lot of code that prints out after each test travis does so it confuses me sometimes. I didn't dislike the feedback from Travis. The only thing i disliked about Travis was that it was only single thread, so waiting for the queue took a while sometimes. The fact that it has a tendency to time out or crash with a fair amount of frequency. I think it's great the way it is I dislike that it's not very straightforward and I have to read through the entire thing to actually find the feedback I'm looking for. It was very difficult to understand where my code was going wrong, there should be an easier way to know what tests failed and what didn't The user interface leaves something to be desired for sure, just a block of logs isn't very easy on the eyes in most cases. I feel like it's very unintuitive and that there should be improvements to layout or at least some visual indicator of what a user is looking at. I didn't dislike anything about it. It was a good tool that just had benefits. But to be honest, I don't know how to read anything besides the parts that listed the test cases. It gave information like 'see report at' or other stuff like that. I just ignored it and looked at the test cases (I don't know if that is the only thing that is beneficial). I don't how you could improve it by. Maybe a couple more test cases. 
-----------------------------------------------------------
Section 2
q2
I looked at the feed back from travis where it said at what point the test failed. Seeing which tests failed and what the expected vs. actual results were was very helpful. I look to see what test failed and try to figure out why it failed. For the labs and projects that usually meant looking at what the output should have been and what it actually was. When I see that my travis test failed, I check what tests failed on travis and I try to fix what happened in my code. It is usually pretty helpful. You are able to see what was passed to it and what is given back. Therefore giving you a good basis as to what you need. On lab 1, travis failed to import the class I was using to round decimals to dollar amounts. It worked perfectly when I ran the program, but travis returned an answer with a bunch of zeroes after the decimal. I was hung up on this issue for hours, and eventually the lab TA just offered to grade it manually. After that I gave up on using travis for feedback. I check where the test failed and was often able to figure out what went wrong with what my code by the output and the expected output. Therefore letting me know what changes I need to make by what the expected output should be. I look what what tests failed and then examine  where Travis says it’s failing. Typically you can easily figure out what’s wrong based on what failed Usually if a check failed I would use the debug build option then go back through my code to see if I can find where I can fix my code. I use the provided information from Travis to look at where the the fault is detected and work back from there to see if the problem is the result of anther error else where in the code. I fix the located problem and resubmit and continue to correct as needed until it passes Travis. I look through Travis to determine which method failed the test case and went to that method in my program to change and edit the faulty code then made another push to determine if I have remedied the problem. I start in the method Travis was testing when it failed, look for dumb typos or logic errors and then slowly move to other methods that could be messing up. I've also pulled the methods out into a separate test file to try to isolate the problem. On the first couple labs, I had a hard time reading the Travis logs to find what my errors were. Later on, the logs became very useful for pointing me towards methods with bugs. Once I received feedback from Travis, I would scroll down to see which test I was failing, what my method returned, and what Travis expected my method to return, then I would go and check the test file to see which test I was actually failing, and what it was asking of my program. Sometimes I would see right away what the problem was and adjust my code. Other times I was unable to get Travis to accept my code as fully functional. When I failed a test case on Travis, depending on the lab I would look at the output my program gave vs the output Travis expected. If I was off by a few decimal places I knew it was a rounding error. Look at the value and what could have been missing. I realize that travis just calls myclass.thismethod and doesn't actually run the program, so I know that something in that specific method doesn't work. However, having to provide us a template that we have to follow almost exactly kind of restricts our code, but I guess it's fine for a beginner class. Also, seeing the input Travis does would be helpful, rather than just the output. I go through travis to see what failed, more specifically, the test case(s) that failed and what line(s) it failed at, then edit my code accordingly. I looked at the feedback from Travis, if a test case created an error (ex. Out of bounds exception) I would fix my code to account for this test case. If Travis was expecting a different outcome, I would edit my code to produce said outcome. This was very helpful when trying to troubleshoot code that I personally thought was finished, but couldn't test extensively. With the feedback from Travis, I was able to see which test(s) failed in my code which is clearly labeled as you scroll down Travis. Granted, these tests were only clear because the methods in labs or projects were already given to us and they matched the method names that Travis would test. Typically, once I found out where I failed, I went to that method (or whatever Travis was checking) and tried to troubleshoot from there. If I had many failed tests, I went through them one by one. If i failed a test case, i would check to see which case failed. Then i'd compare my output vs what Travis said the output should be. That was usually enough to set me on the right track in fixing it 
-----------------------------------------------------------
q3
I liked how it showed the specific tests it was running and where in your code the test failed. For example, it would show you what inputs it gave to a certain method, and it would show what the function returned, and what the function was supposed to return. That made it easier to pinpoint not only which part of the code was wrong, but what it was doing wrong. It was a helpful way to test my code. It is quick and rewarding when you pass your tests. I also like how it will specifically tell me what is wrong with my code. Instant feedback, to see if it meets the minimum running requirements. Never used it/didn't know how to use it. I like that it gives me a good idea about how well I am doing on the assignment I am doing and it checks if I forgot to do a certain aspect of my program. It’s pretty specific in which functions are giving out errors. I like that with Travis  I can know if there are bugs in my code because even though my code compiled in terminal, it may not still run the way I want it to. It allows me to get instant feedback on my work and know if the program will pass the assignments requirements. The nearly instant feedback is absolutely invaluable in finding obvious bugs with the program and the seamless integration with GitHub makes it effortless and easy to use which is just as important in my opinion. Catches those small mistakes you miss when you're rushing to finish an assignment. Also when I turn a lab in and it passes Travis that gives me peace of mind that I'll get a good grade on the lab. Instant and ungraded. We can make test cases if we want, but Travis has been a helpful way to check what our instructor/grader is looking for. In most cases it was nice to see that my code was passing an initial check. It felt good to see the green light up on the travis website. It also helped me catch some small but significant semantic errors. I like how the feedback from Travis leaves out a lot of the guess and check work. Of course I would thoroughly test and debug my program before pushing to GitHub/Travis, however sometimes the feedback from Travis was very valuable in fixing certain outputs. It's nice to check our code, similar to practiceit. It's very quick for the most part, instant results, I don't have to manually input and check outputs to see if my code is working properly. I loved how quick it was compared to receiving personal feedback from a TA for example. The satisfaction of seeing the green 'passed' check mark. Also the implementation with github Having any feedback is great, it was nice knowing what parts of my code were failing to meet specifications. What i found most helpful was what travis said my output was vs what it should be. 
-----------------------------------------------------------
q4
Sometimes it wasn't as specific, like certain "Out of Bounds" errors for example. But that isn't really specific to travis because that happens on the terminal too. Overall, I thought travis was very helpful and definitely worth it. Sometimes it seemed Travis would get a different result that what my IDE was producing. I don't know if it was the fault of Travis or my IDE (Eclipse) though. There were a couple times where I fixed my code so it would work, but travis still would not accept it because of the way it was programmed. This happened with Sudoku so my team had to spend time to program in a way that travis would be happy with, even though the program worked fine. I wish it gave you more detailed feedback on why test cases were failing. Make sure the grading actually works and make sure students know how to use it, because I never did. Sometimes it takes a while for the job to be processed onto Travis even when it is unlikely that everyone is pushing to Travis at the same time. It can sometimes take too long to run, and when I first started using it it was a little confusing to use. I felt like sometimes the debug build option on Travis took too long to run. Also felt like Travis could be more concise and clear about why checks failed. When multiple students submit at once the queue for feedback can result in long wait times. The feedback provided was occasionally redundant or unclear. Some of the test cases provide error codes that are not very helpful especially in boolean methods. It can be difficult to find the test cases sometimes. If there was a title over where the test cases start that would be helpful. Allowing multiple threads to be run simultaneously. With Project 2, there was also an error in the Travis tests that caused it to run infinitely if one of the tests failed. This gave us little or no feedback because Travis would just time out instead of finishing. I would have liked a better tutorial on how to use it. I didn't really start utilizing it until the 4th lab because I hadn't made the connection between the test file and where exactly to find out what Travis was able to tell me. It also needs to work perfectly, for it to be helpful. It is easy to put a lot of stock in Travis, especially considering how often it is stated that "we will not get a good grade if we don't pass Travis." So if a bug occurs and is throwing a red flag when it shouldn't be, I can see a lot of time being wasted, as students second guess their own coding over what Travis says. Possibly a more friendly UI, sometimes test cases were hard to locate. Can't see the input. Travis isn't the endgame grader, so it's fine if someone thinks they can cheat just using the specific values travis provides to get a good grade, because they won't. Also, my code on Sudoku timed out without giving a response, although I'm not sure why because it worked fine on my computer, so calling myclass.thismethod() isn't the best way to check grades in my opinion, running the entire program rather would be better. It would be nice if travis was more user-friendly and highlighted or only showed the test cases and what failed/passed. Scrolling to find it is a bit of a nuisance. The build times were too long for the amount of students using it. Sometimes I would be on queue for almost an hour waiting to receive feedback. 0 I understand coding it on your end might be laborious, but more test cases would be nice. In some labs/ projects, not every part of our code was tested. For example, error handling might be something worth having travis test for. Otherwise, I found travis to be a crucial tool in debugging throughout this course. 
-----------------------------------------------------------
Section 3
q2
I see what the expected output is compared to my output and see what the difference is and what about my code might be causing it I usually check what travis has to say and according to the feedback I usually look at the function where the problem is occurring and try to troubleshoot it. I check through Travis and see what tests failed when i tried to run my program through it. I looked at where the case failed (which method) and then went back to my code and tried to debug the problem. I would scroll to find where the program went wrong, then I would fix the error and push again. I go back to the method(s) the Travis test deals with, and then do a more in depth test to try to identify the problem. Once I feel I know what is going wrong, I formulate a solution and implement it, followed by another round of testing. I    would find the test that my program was failing, and find where in my program that was testing. I    would then perform some testing on my own to see if I    could fix the code and then resubmit to travis to see if it was still failing. If it was still failing, repeat. When a test fails on Travis I look to see if there is a specific test that fails and why. If the result is different from what I expect then I trace it back to my code and test it further. I would look on Travis for what test I failed and then proceeded to alter my code and then test again. I repeated this until the code passes all the Travis  tests and fulfills all the requirements of the lab. I find where the error occurred, then attempt to figure it out why it failed. I try to fix that area of my code then push it again to see if it's fixed. The first place I would look for what was wrong was not Travis but my code. I personally found finding errors on Travis to be very difficult, just finding where they were. Once I found them it was easy to figure out what went wrong and why. 
-----------------------------------------------------------
q3
It tests inputs that I may not have thought of It is very helpful, because sometimes you think the lab or project is working, but once you put it into travis you can see that there is a test case for which the code doesn't work. It helps narrow down the problem and it very helpful. It is very helpful for figuring out what parts of my code need to be fixed to make it complete and working before I turn it in. Travis told me the method in which my case failed which made it much easier for me to fix my code. it let me see exactly what kind of situations I would be graded on and would evaluate specific modules. It is very helpful when it displays the test, along with what the correct output was/what my code output. This way, I can compare the two, which can be helpful when trying to identify the problem. It showed where there was a problem and what the problem could be caused by. I really like that it provides a sense of how I will score on a lab before I turn it in. It easily picks up on small mistakes that I may miss. It is almost like having a peer reviewer. I also like that it shows each test case and the outcome. The feedback was rather quick, granted I would try to finish the lab way before the due date. The explanation for why my code failed a test case was simple to understand. I like that it tells me where the error occurred so I can focus my time on certain sections of the code. It was nice to know whether or not my code works the way that you wanted it to work. If I had all the methods doing what they were supossed to do. And also it was nice not having to test it as in-depth because a computer would do that for me, I did test it but I didn't run through all of the different types of problems. 
-----------------------------------------------------------
q4
Sometimes it says that my output has a difference than the expected but there was no difference, I don’t know how to fix that. It is kind of hard to read because there is so much stuff going on in the feedback window that it is hard to narrow down where travis is giving you feedback and where the random code is. There have been a couple occasions where I have ran through my code and it isn't passing through travis because I have created methods that don't use parts of the methods that Travis is testing. In these cases my code still works and is complete but travis thinks it doesn't since it is getting a different response than it thinks it should. I honestly can't think of a way to improve it because the way Travis reported what the problem was, meaning it gave me what my code did and what my code should have done was extremely helpful. I can't think of a way to improve the feedback without making the solution obvious to the student. finding the error could be difficult as you would have to scroll past all of the VM info at the bdginning It seems that feedback is only displayed when a test fails. It could be a helpful reaffirmation of your thinking process if the tests were always displayed, allowing you confirm successful/failed test cases. It was kind of slow sometimes, but I    think that's okay given that it shouldn't be the only way that you were testing your code. I     don't have any real complaints about it. Sometimes it takes awhile to finish testing, but I do not really mind this. Sometimes the code above the test cases can be a little confusing, but otherwise it is not hard to navigate. One thing that would help is making the the failed test case/error more visibly noticeable within the Travis feedback. Scrolling through the Travis feedback to try and find the failed test case was a little tedious. Sometimes it's hard to find the error, as there's a lot to scroll through. Also, sometimes it takes a good amount of time to check but I'm not sure that can be avoided. Finding where Travis stared testing your program, and where to find the errors, was challenging. A way to fix it is if you could put in something along the lines of "[]---Your Testing Starts here---[]" would be helpful. 
-----------------------------------------------------------
